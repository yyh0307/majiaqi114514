import sounddevice as sd
import numpy as np
import threading
import queue
import torch
from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq

# ===================== æ ¸å¿ƒé…ç½®ï¼ˆæç®€ï¼‰ =====================
SAMPLING_RATE = 16000  # Qwen3-ASR-Flashå›ºå®šè¦æ±‚16000é‡‡æ ·ç‡
CHUNK_DURATION = 2     # æ¯2ç§’è½¬å†™ä¸€æ¬¡ï¼ˆå¯è°ƒï¼Œè¶Šå°è¶Šå®æ—¶ï¼‰
MODEL_NAME = "Qwen/Qwen3-ASR-Flash"
LANGUAGE = "zh"        # æŒ‡å®šä¸­æ–‡è½¬å†™ï¼Œæå‡å‡†ç¡®ç‡

# ===================== åˆå§‹åŒ– =====================
# åŠ è½½Qwen3-ASR-Flashæ¨¡å‹å’Œå¤„ç†å™¨
print(f"æ­£åœ¨åŠ è½½{MODEL_NAME}æ¨¡å‹...")
processor = AutoProcessor.from_pretrained(MODEL_NAME)
model = AutoModelForSpeechSeq2Seq.from_pretrained(MODEL_NAME)
model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

# éŸ³é¢‘é˜Ÿåˆ—ï¼šå­˜å‚¨é‡‡é›†çš„éŸ³é¢‘æ•°æ®
audio_queue = queue.Queue()

# ===================== éŸ³é¢‘é‡‡é›†å›è°ƒ =====================
def collect_audio(indata, frames, time, status):
    """éº¦å…‹é£é‡‡é›†å›è°ƒï¼Œç›´æ¥å­˜åŸå§‹éŸ³é¢‘æ•°æ®"""
    if status:
        print(f"é‡‡é›†æç¤ºï¼š{status}", flush=True)
    # è½¬æ¢ä¸ºQwen3-ASR-Flashè¦æ±‚çš„æ ¼å¼ï¼ˆå•å£°é“ã€float32ï¼‰
    audio_data = indata[:, 0].astype(np.float32)
    audio_queue.put(audio_data)

# ===================== å®æ—¶è½¬å†™çº¿ç¨‹ =====================
def transcribe_real_time():
    """æŒç»­ä»é˜Ÿåˆ—å–éŸ³é¢‘å¹¶è½¬å†™ï¼Œæ— ffmpegä¾èµ–"""
    print(f"âœ… å¼€å§‹å®æ—¶è½¬å†™ï¼ˆ{CHUNK_DURATION}ç§’/æ®µï¼‰ï¼ŒæŒ‰Ctrl+Cåœæ­¢...")
    while True:
        # æ”¶é›†æŒ‡å®šæ—¶é•¿çš„éŸ³é¢‘æ•°æ®
        audio_chunks = []
        target_frames = int(SAMPLING_RATE * CHUNK_DURATION)  # ç›®æ ‡æ€»å¸§æ•°
        collected_frames = 0
        
        while collected_frames < target_frames:
            try:
                chunk = audio_queue.get(timeout=1)
                audio_chunks.append(chunk)
                collected_frames += len(chunk)
            except queue.Empty:
                break
        
        # è½¬å†™æœ‰æ•ˆéŸ³é¢‘
        if audio_chunks:
            # æ‹¼æ¥å¹¶å½’ä¸€åŒ–ï¼ˆQwen3-ASR-Flashå¿…éœ€æ­¥éª¤ï¼‰
            audio = np.concatenate(audio_chunks)
            audio = audio / np.max(np.abs(audio)) if np.max(np.abs(audio)) > 0 else audio
            
            # æ ¸å¿ƒè½¬å†™ï¼šä½¿ç”¨Qwen3-ASR-Flashæ¨¡å‹
            inputs = processor(audio, sampling_rate=SAMPLING_RATE, return_tensors="pt")
            with torch.no_grad():
                outputs = model.generate(**inputs)
            result_text = processor.decode(outputs[0], skip_special_tokens=True)
            
            if result_text.strip():
                print(f"ğŸ“ è½¬å†™ç»“æœï¼š{result_text}")

# ===================== å¯åŠ¨ç¨‹åº =====================
if __name__ == "__main__":
    # å¯åŠ¨è½¬å†™çº¿ç¨‹
    transcribe_thread = threading.Thread(target=transcribe_real_time, daemon=True)
    transcribe_thread.start()
    
    # å¯åŠ¨éº¦å…‹é£é‡‡é›†ï¼ˆç›´æ¥è¿æ¥ç¡¬ä»¶é©±åŠ¨ï¼Œæ— ä¸­é—´æ–‡ä»¶ï¼‰
    with sd.InputStream(
        samplerate=SAMPLING_RATE,
        channels=1,  # å•å£°é“
        callback=collect_audio,
        blocksize=1024  # é‡‡é›†å—å¤§å°ï¼Œé€‚é…ç¡¬ä»¶
    ):
        try:
            input()  # é˜»å¡ä¸»çº¿ç¨‹ï¼Œä¿æŒç¨‹åºè¿è¡Œ
        except KeyboardInterrupt:
            print("\nğŸ›‘ è½¬å†™å·²åœæ­¢")